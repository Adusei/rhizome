*******************
Backend Source Code
*******************

.. contents:: Table of Contents

Standardization and Validation of Information
=============================================

The polio backend is based around the **datapoint** table which has a key for location, indicator and campaign.  The datapoint table gathers and consolidates information from Data Entry, CSV upload, ETL of 3rd party APIs and Data Sources and makes this information available to a REST api.  This api powers the dashboards, as well as the data explorer.

The system differentiates between conflicts and information from different sources via the **source_datapoint** table which holds information uploaded directly from spreadsheets.

In addition the system maintains **source_indicators** , **source_locations**, and **source_campaigns** all of which have cooresponding mapping tables.  Each of the source tables have a cooresponding **document_id** which gives a link to the source of the raw data.

When a source_datapoint has mappings for location, campaign, and indicator, by using the **refresh_master** method for that document_id will create, or update ( if a conflicting datapoint exists ) the datapoint table.

There are two ways that information gets into the system.

**Data Entry Form**
**ETL Process**



Data Entry Form  (see DataEntryResource)
========================================
- Accepts POST request that perform inserts and updates on the datpaoint table
- Unlike source_datapoints ( as you'll learn more about below ) information
  that comes through the Data Entry Api is stored in the audit_table.
- **An Important Point** - Information from the Data Entry form always takes
  precedence over information flowing in from source_datapoints.  A Human
  should always have the ability to override what has been automatically
  generated by the system.

ETL Process
===========

The flow of data in the ETL Process is as follows:

- Each file, or incoming data stream has a corresponding *document_id*
- Each fact in a document is then translated to a *source_datapoint* which
  contains principally a *campaign_string*, *location_string*,
  *indicator_string* and *value*.

RefreshMaster
-------------

- The RefreshMaster class is instantiated for a document_id which in turn:
- Creates an necessary meta_data rows (ex. source_indicator) from the
  raw data.
- **INSERT** a datapoint for each source_datapoint that has a corresponding
  mapping for the location, campaign, and indicator.
- **UPDATE** datapoints only in the case when that original datapoint was
  the result of another source_datapoint. That is, this method will NOT
  override data that was inserted via the data_entry form (a datapoint that
  has been inserted from the data_entry form will have the
  source_datapoint_id =-1).
- **DELETE** datapoints for which a mapping has expired.  If the mappings
   that were the result of that document_id
      - so if you load a csv with 10 source_datapoints, map everything,
        then refresh master you should see 10 datapoints.
      - if then you delete the mapping for a location in this document that
        has attached to it 2 datapoints, and refreshed master, you would
        then see that the document_id has 8 datapoints instead of 10.

Mapping
-------

- The general principal is that, the system creates a source_metadata record
  only when the system has never seen that string before.
- If the source_meta_data_id has been mapped by a user on a previous document
  then the data should flow for that particular meta data item without the user
  having to map that meta data.
- For a document_id, if a source_metadata row is created, or has been created
  from another document_id but not mapped, the user will be prompted to map
  this metadata when visiting the document_review page
  ''/source_data/document_review/<document_id>''

Regarding POLIO-205 - How are we to determine the semantic identities of locations in a complex locational heirarchy.

Consider the following wikipedia page:


http://en.wikipedia.org/wiki/Killa_Abdullah_District

  *"Killa Abdullah or Qilla Abdullah or Abdullah Qilla (Pashto: قلعہ عبد الله‎)"...*

In our system we map data from each source, to semantic identites in our system.  For instance, "# vaccinated" is the same as "num vaccinated," and "NG JUN 2014" is the same as "Nigeria June 2014."

When ingesting a spreadsheet, here are the rules as to how datapoints are mapped and validated.
  - each souce_datapoint must have a record explicilty mapping the indicator_string, campaign_string and location_code to their respective IDs.
  - locations are not Auto mapped on their Name, but rather their location code.

**location Codes**

*NIGERIA*
  - WHO has a the following naming convention for Settlements in Nigeria:
      ''<province><district><sub-district><settlement>''
  - Bo created existing sub-districts with this convention so i was able to map a large part of the ODK data using this convention.

*AFGHANISTAN*
  - I am not sure we need to ask Bo

*PAKISTAN*
  - I am not sure we need to ask Bo

Needs Documentation
-------------------
- Shape File ingestion
- transforming data into source_datapoints
    ->CSV pivoted
    ->CSV Non Pivoted
    ->ODK

Future Topics Regarding locations
-------------------------------
  - when boundaries change over time
  - outbreak countries and new office_ids
  - Storing Health Camp Data


Caching
=======

Datapoints are stored at four levels.  Each represent a database table as well
as a stage in the cache process.  The data from each step of the aggregation /
calculation cycle are available to you for debugging missing and incorrect
information

- ``datapoint`` - the level at which raw data is stored
- ``agg_datapoint`` - raw data aggregated locationally.
- ``datapoint_with_computed`` - both raw and aggregated data stored including
  data for calculated indicators.
- ``datapoint_abstracted`` - the aggregated and calculated data stored in a
  format that mimics the response format of the ``api/v1/datapoint`` API.

The Cache is refresh by instatiating the AggRefresh Object.

For example:
  .. code-block:: python

    from rhizome.agg_tasks import AggRefresh

    ## refresh the cache with the default behavior
    cr = AggRefresh()
    print cr.status

    >> 'SUCCESS'

Or In the case where you want to refresh the cache for a list of datapoint_ids:
  .. code-block:: python

    from rhizome.agg_tasks import AggRefresh
    from rhizome.models import DataPoint

    ## get a List of DataPoint IDs for the location Arghestan ##
    dp_ids = DataPoint.objects.filter(location_id = 13317).values_list('id',flat=True)

    ## refresh the cache for the datapoint_ids retrieved above ##
    cr = AggRefresh(datapoint_id_list = dp_ids)
    print cr.status

    >> 'SUCCESS'

The Cache Refresh Class
-----------------------

  .. autoclass:: rhizome.agg_tasks.AggRefresh

**When the __init__() method is called two subsequent methods are called:**

  - ``set_up()`` - get all metadata required to refresh cache
  - ``main()`` - aggregate, calculatd and save new information

the set_up() method
-------------------
  .. automethod:: rhizome.agg_tasks.AggRefresh.set_up

  .. automethod:: rhizome.agg_tasks.AggRefresh.get_indicator_ids

  .. automethod:: rhizome.agg_tasks.AggRefresh.get_datapoints_to_cache

the main() method
-----------------

  .. automethod:: rhizome.agg_tasks.AggRefresh.main

  .. automethod:: rhizome.agg_tasks.AggRefresh.agg_datapoints

  .. automethod:: rhizome.agg_tasks.AggRefresh.calc_datapoints




Permissions
===========

- The permissioning system is based mainly on django's authentication
  system with an extension using django-gaurdian that allows for object
  level permissions.
- Django has no built in resources for creating "view" permissions,
  currently "view" permissions are handled by django gaurdian.

PERMISSIONS SCHEMA
    - auth_permissions
    - auth_user
    - auth_group
    - auth_user_permission
    - auth_group_permission


QA Calculation and Aggregation
==============================

Google Doc
Testing Expected Data


Reference Sheet
===============

Here are some terms you should get familiar with when working in this
application.

- document_id
- source_datapoint
- datapoint
- location
  locations have a parent, lon / lat, location type
  **uniqueness for location is defined by location_name, location_type, country**
  Prior we had an issue in which two locations with the same name ( HRA Level ) and in our ingestion we collapsed both locations into one, causing locational aggregation to break and display conflicting data.
  We also had an issue in which a location in the same country has the same name but with a different location type ( sokoto settlement vs. sokoto state).
  We will also be storing a location_geo_json table that will hold location_id, geo_json ( as a blob )

- indicator
- campaign
- map
- agg_datapoint
- datapoint_with_computed
- calculated_indicator_component
- etl_job
- audit_table


Aggregation by location
---------------------

If you request a location for which there is no data, the system will traverse the
hierarchy of locations down and aggregate certain data that it finds at those levels by
adding them together. For example, if you request the "Number of Missed
Children" for Nigeria, but that indicator is not stored in the database for
Nigeria, the system will iterate over the states that comprise Nigeria and add
the values it finds for that indicator together. For each state that does not
have a value, it will check its constituent locations, and so on until it finds a
location with a value for that indicator or it runs out of sub-locations to check.

.. image:: img/geo_agg.png

Note that the aggregation behavior varies depending on the indicator datatype.
  - Integer type indicators will have data aggregated by location as above.
  - Percent type indicators will not be aggregated
  - Boolean type indicators for lower level locations, such as districts, will be aggregated as percents when propagated to upper level locations. For example, a province with 5 districts with evening meetings and and 5 without evening meetings, would have a 50% value overall for evening meetings.

If the value of an indicator was generated by aggregating data from sub-locations,
the indicator object will have an ``is_agg`` property:

.. code-block:: json

  ...
  location: 23,
  indicators: [{
    indicator: 1,
    value: ...
  }, {
    indicator: 2,
    value: ...,
    is_agg: true
  }]
  ...

In the above example, a value for indicator 1 was found for location 23. No value
for indicator 2 was found for location 23, so the system calculated that value by
aggregating the values of it sub-locations.

Conflicts with Sub-locations
++++++++++++++++++++++++++

If a value is stored for a given location, that is the value returned regardless
of whether or not the location's sub-locations also have values. Because there is
nothing preventing a value being stored for a location and its sub-locations, it is
possible that the stored values at differing levels may conflict.

.. image:: img/geo_agg_conflict.png

In the above example one of the locations has a stored value of 7, and its three
sub-locations have values of 1, 1, and 3. This could be indicative of an error in
the data and should be flagged. Regardless of whether this is an error or
intentional, the value returned for that location (and the value used in
aggregation for any of its parent locations) is the value stored for the location;
the values in the sub-locations are ignored except when they are explicitly
requested.

Partial Missing Values
++++++++++++++++++++++

When aggregating data geographically, it is possible to calculate the value for
a location even if not all of its sub-locations have data.

.. image:: img/geo_agg_partial.png

These situations should be flagged so that users are aware of them when they
occur. It's important to know that the value for the country you are seeing is
actually only representative of some portion of its sub-locations and not the
entire country.

Aggregation by calculation
---------------------
Certain indicators are calculated from other indicators. For example, "total missed children" is the sum of "missed due to absence" and "missed due to refusal". Though "total missed children" may not exist in a source submission, an agg task can calculate its value based on its child indicators. Calculated indicators can be generated by designating child indicators in CalculatedIndicatorComponent objects. Calculation types of CalculatedIndicatorComponents include 'PART_TO_BE_SUMMED' for calculated indicators that are generated as a sum, and 'NUMERATOR' and 'DENOMINATOR' CalculatedIndicatorComponents for generating percent indicators. Similarly components can be designated as 'PART_OF_DIFFERENCE' and 'WHOLE_OF_DIFFERENCE' for the percentage calculation: 

        WHOLE_OF_DIFFERENCE(x) - PART_OF_DIFFERENCE(y)
        -----------------------------------------
             WHOLE_OF_DIFFERENCE(x)

Conflicts with calculation aggregation and partial missing values
++++++++++++++++++++++
Similar to location aggregation, an aggregated indicator value will be overridden if the indicator value has been set explicitly. Also, for calculated indicators that are computed as a sum, if any 'PART_TO_BE_SUMMED' component is missing a datapoint, the sum skips over this component. For example, if a datapoint value exists for the indicator 'missed due to absence', but not for 'missed due to refusal', the 'total missed childen' indicator sum will be the value of 'missed due to absence'.

Controlling Aggregation Behavior
++++++++++++++++++++++++++++++++

You can control the behavior of the aggregation using the ```` parameter.

``mixed``
  default

  If the requested location has stored data, use that, otherwise travers the sub-
  locations to aggregate the indicators found there

``agg-only``
  Only return data aggregated from sub-locations. If the location you requested
  actually has data stored on it, it will be ignored

``no-agg``
  Do not travers the sub-locations to aggregate data if the requested location does
  not have a value stored
