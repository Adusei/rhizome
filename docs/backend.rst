*******************
Backend Source Code
*******************

.. contents:: Table of Contents

Standardization and Validation of Information
=============================================

The polio backend is based around the **datapoint** table which has a key for region, indicator and campaign.  The datapoint table gathers and consolidates information from Data Entry, CSV upload, ETL of 3rd party APIs and Data Sources and makes this information available to a REST api.  This api powers the dashboards, as well as the data explorer.

The system differentiates between conflicts and information from different sources via the **source_datapoint** table which holds information uploaded directly from spreadsheets.

In addition the system maintains **source_indicators** , **source_regions**, and **source_campaigns** all of which have cooresponding mapping tables.  Each of the source tables have a cooresponding **document_id** which gives a link to the source of the raw data.

When a source_datapoint has mappings for region, campaign, and indicator, by using the **refresh_master** method for that document_id will create, or update ( if a conflicting datapoint exists ) the datapoint table.

There are two ways that information gets into the system.

**Data Entry Form**
**ETL Process**



Data Entry Form  (see DataEntryResource)
========================================
- Accepts POST request that perform inserts and updates on the datpaoint table
- Unlike source_datapoints ( as you'll learn more about below ) information
  that comes through the Data Entry Api is stored in the audit_table.
- **An Important Point** - Information from the Data Entry form always takes
  precedence over information flowing in from source_datapoints.  A Human
  should always have the ability to override what has been automatically
  generated by the system.

ETL Process
===========

The flow of data in the ETL Process is as follows:

- Each file, or incoming data stream has a corresponding *document_id*
- Each fact in a document is then translated to a *source_datapoint* which
  contains principally a *campaign_string*, *region_string*,
  *indicator_string* and *value*.

RefreshMaster
-------------

- The RefreshMaster class is instantiated for a document_id which in turn:
- Creates an necessary meta_data rows (ex. source_indicator) from the
  raw data.
- **INSERT** a datapoint for each source_datapoint that has a corresponding
  mapping for the region, campaign, and indicator.
- **UPDATE** datapoints only in the case when that original datapoint was
  the result of another source_datapoint. That is, this method will NOT
  override data that was inserted via the data_entry form (a datapoint that
  has been inserted from the data_entry form will have the
  source_datapoint_id =-1).
- **DELETE** datapoints for which a mapping has expired.  If the mappings
   that were the result of that document_id
      - so if you load a csv with 10 source_datapoints, map everything,
        then refresh master you should see 10 datapoints.
      - if then you delete the mapping for a region in this document that
        has attached to it 2 datapoints, and refreshed master, you would
        then see that the document_id has 8 datapoints instead of 10.

Mapping
-------

- The general principal is that, the system creates a source_metadata record
  only when the system has never seen that string before.
- If the source_meta_data_id has been mapped by a user on a previous document
  then the data should flow for that particular meta data item without the user
  having to map that meta data.
- For a document_id, if a source_metadata row is created, or has been created
  from another document_id but not mapped, the user will be prompted to map
  this metadata when visiting the document_review page
  ''/source_data/document_review/<document_id>''

Regarding POLIO-205 - How are we to determine the semantic identities of Regions in a complex regional heirarchy.

Consider the following wikipedia page:


  http://en.wikipedia.org/wiki/Killa_Abdullah_District

  *"Killa Abdullah or Qilla Abdullah or Abdullah Qilla (Pashto: قلعہ عبد الله‎)"...*

In our system we map data from each source, to semantic identites in our system.  For instance, "# vaccinated" is the same as "num vaccinated," and "NG JUN 2014" is the same as "Nigeria June 2014."

When ingesting a spreadsheet, here are the rules as to how datapoints are mapped and validated.
  - each souce_datapoint must have a record explicilty mapping the indicator_string, campaign_string and region_code to their respective IDs.
  - Regions are not Auto mapped on their Name, but rather their region code.

**Region Codes**

*NIGERIA*
  - WHO has a the following naming convention for Settlements in Nigeria:
      ''<province><district><sub-district><settlement>''
  - Bo created existing sub-districts with this convention so i was able to map a large part of the ODK data using this convention.

*AFGHANISTAN*
  - I am not sure we need to ask Bo

*PAKISTAN*
  - I am not sure we need to ask Bo

Needs Documentation
-------------------
- Shape File ingestion
- transforming data into source_datapoints
    ->CSV pivoted
    ->CSV Non Pivoted
    ->ODK

Future Topics Regarding Regions
-------------------------------
  - when boundaries change over time
  - outbreak countries and new office_ids
  - Storing Health Camp Data


Caching
=======

Datapoints are stored at four levels.  Each represent a database table as well
as a stage in the cache process.  The data from each step of the aggregation /
calculation cycle are available to you for debugging missing and incorrect
information

- ``datapoint`` - the level at which raw data is stored
- ``agg_datapoint`` - raw data aggregated regionally.
- ``datapoint_with_computed`` - both raw and aggregated data stored including
  data for calculated indicators.
- ``datapoint_abstracted`` - the aggregated and calculated data stored in a
  format that mimics the response format of the ``api/v1/datapoint`` API.

The Cache is refresh by instatiating the CacheRefresh Object.

For example:
  .. code-block:: python

    from datapoints.cache_tasks import CacheRefresh

    ## refresh the cache with the default behavior
    cr = CacheRefresh()
    print cr.status

    >> 'SUCCESS'

Or In the case where you want to refresh the cache for a list of datapoint_ids:
  .. code-block:: python

    from datapoints.cache_tasks import CacheRefresh
    from datapoints.models import DataPoint

    ## get a List of DataPoint IDs for the region Arghestan ##
    dp_ids = DataPoint.objects.filter(region_id = 13317).values_list('id',flat=True)

    ## refresh the cache for the datapoint_ids retrieved above ##
    cr = CacheRefresh(datapoint_id_list = dp_ids)
    print cr.status

    >> 'SUCCESS'

The Cache Refresh Class
-----------------------

  .. autoclass:: datapoints.cache_tasks.CacheRefresh

**When the __init__() method is called two subsequent methods are called:**

  - ``set_up()`` - get all metadata required to refresh cache
  - ``main()`` - aggregate, calculatd and save new information

the set_up() method
-------------------
  .. automethod:: datapoints.cache_tasks.CacheRefresh.set_up

  .. automethod:: datapoints.cache_tasks.CacheRefresh.get_indicator_ids

  .. automethod:: datapoints.cache_tasks.CacheRefresh.get_datapoints_to_cache

the main() method
-----------------

  .. automethod:: datapoints.cache_tasks.CacheRefresh.main

  .. automethod:: datapoints.cache_tasks.CacheRefresh.agg_datapoints

  .. automethod:: datapoints.cache_tasks.CacheRefresh.calc_datapoints




Permissions
===========

- The permissioning system is based mainly on django's authentication
  system with an extension using django-gaurdian that allows for object
  level permissions.
- Django has no built in resources for creating "view" permissions,
  currently "view" permissions are handled by django gaurdian.

PERMISSIONS SCHEMA
    - auth_permissions
    - auth_user
    - auth_group
    - auth_user_permission
    - auth_group_permission


QA Calculation and Aggregation
==============================

Google Doc
Testing Expected Data


Reference Sheet
===============

Here are some terms you should get familiar with when working in this
application.

- document_id
- source_datapoint
- datapoint
- region
  Regions have a parent, lon / lat, region type
  **uniqueness for region is defined by region_name, region_type, country**
  Prior we had an issue in which two regions with the same name ( HRA Level ) and in our ingestion we collapsed both regions into one, causing regional aggregation to break and display conflicting data.
  We also had an issue in which a region in the same country has the same name but with a different region type ( sokoto settlement vs. sokoto state).
  We will also be storing a region_geo_json table that will hold region_id, geo_json ( as a blob )

- indicator
- campaign
- map
- agg_datapoint
- datapoint_with_computed
- calculated_indicator_component
- etl_job
- audit_table

Computed vs Stored Indicators
-----------------------------

Computed indicators are not stored in the database, they are calculated from
other indicators in the database. For example, the "Percentage of Missed
Children" indicator is computed by dividing the "Number of Missed Children"
indicator by the "Number of Targetd Children" indicator.

Computed indicators are fetched using the same ``/api/v1/datapoint/`` endpoint
as stored indicators.

The response from the ``/api/v1/indicator/`` endpoint for a computed indicator
will include an additional property not included in a stored indicator:
``computed_from``.

.. code-block:: json

  {
    meta: {...},
    objects:[{
      ...
      computed_from: [...]
    }],
    errors: {...}
  }

The ``computed_from`` property is an array of references to the indicators used
to compute this one. The format of the references depends on the ``uri_format``
parameter.

Aggregation by Region
---------------------

If you request a region for which there is no data, the system will traverse the
hierarchy of regions down and aggregate the data it finds at those levels by
adding them together. For example, if you request the "Number of Missed
Children" for Nigeria, but that indicator is not stored in the database for
Nigeria, the system will iterate over the states that comprise Nigeria and add
the values it finds for that indicator together. For each state that does not
have a value, it will check its constituent regions, and so on until it finds a
region with a value for that indicator or it runs out of sub-regions to check.

.. image:: img/geo_agg.png

If the value of an indicator was generated by aggregating data from sub-regions,
the indicator object will have an ``is_agg`` property:

.. code-block:: json

  ...
  region: 23,
  indicators: [{
    indicator: 1,
    value: ...
  }, {
    indicator: 2,
    value: ...,
    is_agg: true
  }]
  ...

In the above example, a value for indicator 1 was found for region 23. No value
for indicator 2 was found for region 23, so the system calculated that value by
aggregating the values of it sub-regions.

Conflicts with Sub-regions
++++++++++++++++++++++++++

If a value is stored for a given region, that is the value returned regardless
of whether or not the region's sub-regions also have values. Because there is
nothing preventing a value being stored for a region and its sub-regions, it is
possible that the stored values at differing levels may conflict.

.. image:: img/geo_agg_conflict.png

In the above example one of the regions has a stored value of 7, and its three
sub-regions have values of 1, 1, and 3. This could be indicative of an error in
the data and should be flagged. Regardless of whether this is an error or
intentional, the value returned for that region (and the value used in
aggregation for any of its parent regions) is the value stored for the region;
the values in the sub-regions are ignored except when they are explicitly
requested.

Partial Missing Values
++++++++++++++++++++++

When aggregating data geographically, it is possible to calculate the value for
a region even if not all of its sub-regions have data.

.. image:: img/geo_agg_partial.png

These situations should be flagged so that users are aware of them when they
occur. It's important to know that the value for the country you are seeing is
actually only representative of some portion of its sub-regions and not the
entire country.

Controlling Aggregation Behavior
++++++++++++++++++++++++++++++++

You can control the behavior of the aggregation using the ```` parameter.

``mixed``
  default

  If the requested region has stored data, use that, otherwise travers the sub-
  regions to aggregate the indicators found there

``agg-only``
  Only return data aggregated from sub-regions. If the region you requested
  actually has data stored on it, it will be ignored

``no-agg``
  Do not travers the sub-regions to aggregate data if the requested region does
  not have a value stored
